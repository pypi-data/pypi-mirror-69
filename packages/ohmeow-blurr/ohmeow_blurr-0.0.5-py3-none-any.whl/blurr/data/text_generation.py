# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01e_data-text-generation.ipynb (unless otherwise specified).

__all__ = ['HF_TextGenerationInput']

# Cell
import ast
from functools import reduce

import torch
from transformers import *
from fastai2.text.all import *

from ..utils import *
from .core import *

# Cell
class HF_TextGenerationInput(list): pass

# Cell
@typedispatch
def build_hf_input(task:ForConditionalGenerationTask, tokenizer, a_tok_ids, b_tok_ids=None, targets=None,
                   max_length=512, pad_to_max_length=True, truncation_strategy='longest_first', **kwargs):

    enc_res = build_hf_input(None, tokenizer, a_tok_ids, b_tok_ids, targets,
                             max_length, pad_to_max_length, truncation_strategy)

    trg_max_length = kwargs['trg_max_length'] if ('trg_max_length' in kwargs) else max_length

    if (len(targets) > 0):
        trg = tokenizer.prepare_for_model(targets[0].tolist(), None,
                                          max_length=trg_max_length,
                                          add_special_tokens=False,
                                          return_tensors='pt')

        pad_id, bos_id, eos_id = tokenizer.pad_token_id, tokenizer.bos_token_id, tokenizer.eos_token_id
        trg_ids = torch.cat((tensor(bos_id)[None], trg['input_ids'][0], tensor(eos_id)[None]))
        trg_ids = F.pad(trg_ids, pad=(0, trg_max_length), value=pad_id)[:trg_max_length+1]

        # the "decoder_input_ids" are the target ids shifted to the right by one (being with pad token)
        enc_res[0].append(trg_ids[:-1])
        target_ids = trg_ids[1:]
    else:
        target_ids = None

    return HF_TextGenerationInput(enc_res[0]), tuplify(target_ids)

# Cell
@typedispatch
def show_batch(x:HF_TextGenerationInput, y, samples, hf_tokenizer, skip_special_tokens=True,
               ctxs=None, max_n=6, **kwargs):
    res = L()
    for inp, trg in zip(x[0], y):
        txt_inp = hf_tokenizer.decode(inp, skip_special_tokens=skip_special_tokens).replace(hf_tokenizer.pad_token, '')
        txt_trg = hf_tokenizer.decode(trg, skip_special_tokens=skip_special_tokens).replace(hf_tokenizer.pad_token, '')
        res.append((txt_inp, txt_trg))

    display_df(pd.DataFrame(res, columns=['text', 'target'])[:max_n])
    return ctxs