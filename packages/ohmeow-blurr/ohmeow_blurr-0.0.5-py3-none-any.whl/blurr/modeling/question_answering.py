# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02c_modeling-question-answering.ipynb (unless otherwise specified).

__all__ = ['HF_QstAndAnsModelWrapper', 'HF_QstAndAnsModelCallback', 'MultiTargetLoss']

# Cell
import ast

import torch
from transformers import *
from fastai2.text.all import *

from ..data.all import *
from .core import *

# Cell
class HF_QstAndAnsModelWrapper(HF_BaseModelWrapper):
    """A custom model wrapper for question answer models since we need all the outputs (not just the first)"""
    def forward(self, x):
        model_kwargs, n_inputs = {}, len(x)
        model_kwargs['input_ids'] = x[0]
        if (n_inputs > 1 and self._include_arg('attention_mask', x[1])): model_kwargs['attention_mask'] = x[1]
        if (n_inputs > 2 and self._include_arg('token_type_ids', x[2])): model_kwargs['token_type_ids'] = x[2]
        if (n_inputs > 3 and self._include_arg('cls_index', x[3])): model_kwargs['cls_index'] = x[3]
        if (n_inputs > 4 and self._include_arg('p_mask', x[4])): model_kwargs['p_mask'] = x[4]

        return self.hf_model(**model_kwargs)

# Cell
class HF_QstAndAnsModelCallback(HF_BaseModelCallback):
    """We need to return everything from the model for question/answer tasks"""
    def after_pred(self): self.learn.pred = self.pred

# Cell
class MultiTargetLoss(Module):
    """Provides the ability to apply different loss functions to multi-modal targets/predictions"""
    def __init__(self, loss_classes=[CrossEntropyLossFlat, CrossEntropyLossFlat], loss_classes_kwargs=[{}, {}],
                 weights=[1, 1], reduction='mean'):

        loss_funcs = [ cls(reduction=reduction, **kwargs) for cls, kwargs in zip(loss_classes, loss_classes_kwargs) ]
        store_attr(self, 'loss_funcs, weights')
        self._reduction = reduction

    # custom loss function must have either a reduction attribute or a reduction argument (like all fastai and
    # PyTorch loss functions) so that the framework can change this as needed (e.g., when doing lear.get_preds
    # it will set = 'none'). see this forum topic for more info: https://bit.ly/3br2Syz
    @property
    def reduction(self): return self._reduction

    @reduction.setter
    def reduction(self, v):
        self._reduction = v
        for lf in self.loss_funcs: lf.reduction = v

    def forward(self, outputs, *targets):
        loss = 0.
        for i, loss_func, weights, output, target in zip(range(len(outputs)),
                                                         self.loss_funcs, self.weights,
                                                         outputs, targets):
            loss += weights * loss_func(output, target)

        return loss

    def activation(self, outs):
        acts = [ self.loss_funcs[i].activation(o) for i, o in enumerate(outs) ]
        return acts

    def decodes(self, outs):
        decodes = [ self.loss_funcs[i].decodes(o) for i, o in enumerate(outs) ]
        return decodes


# Cell
@typedispatch
def show_results(x:HF_QuestionAnswerInput, y, samples, outs, hf_tokenizer, skip_special_tokens=True,
                 ctxs=None, max_n=6, **kwargs):
    res = L()
    for inp, start, end, pred in zip(x[0], *y, outs):
        txt = hf_tokenizer.decode(inp, skip_special_tokens=skip_special_tokens).replace(hf_tokenizer.pad_token, '')
        ans_toks = hf_tokenizer.convert_ids_to_tokens(inp, skip_special_tokens=False)[start:end]
        pred_ans_toks = hf_tokenizer.convert_ids_to_tokens(inp, skip_special_tokens=False)[int(pred[0]):int(pred[1])]

        res.append((txt,
                       (start.item(),end.item()), hf_tokenizer.convert_tokens_to_string(ans_toks),
                       (int(pred[0]),int(pred[1])), hf_tokenizer.convert_tokens_to_string(pred_ans_toks)))

    df = pd.DataFrame(res, columns=['text', 'start/end', 'answer', 'pred start/end', 'pred answer'])
    display_df(df[:max_n])
    return ctxs