# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_modeling-core.ipynb (unless otherwise specified).

__all__ = ['hf_splitter', 'HF_BaseModelWrapper', 'HF_BaseModelCallback']

# Cell
import torch
from transformers import *
from fastai2.text.all import *

from ..utils import *
from ..data.core import *

# Cell
def hf_splitter(m):
    """Splits the huggingface model based on various model architecture conventions"""
    model = m.hf_model if (hasattr(m, 'hf_model')) else m
    root_modules = list(model.named_children())
    top_module_name, top_module = root_modules[0]

    groups = L([ m for m_name, m in list(top_module.named_children()) ])
    groups += L([ m for m_name, m in root_modules[1:] ])

    return groups.map(params).filter(lambda el: len(el) > 0)

# Cell
class HF_BaseModelWrapper(Module):
    def __init__(self, hf_model):
        super().__init__()
        self.hf_model = hf_model

        n_fwd_args = self.hf_model.forward.__code__.co_argcount
        self.hf_model_fwd_args = self.hf_model.forward.__code__.co_varnames[:n_fwd_args][1:]

    def forward(self, x):
        model_kwargs, n_inputs = {}, len(x)
        model_kwargs['input_ids'] = x[0]
        if (n_inputs > 1 and self._include_arg('attention_mask', x[1])): model_kwargs['attention_mask'] = x[1]
        if (n_inputs > 2 and self._include_arg('token_type_ids', x[2])): model_kwargs['token_type_ids'] = x[2]

        return self.hf_model(**model_kwargs)

    def _include_arg(self, arg_name, tensor_val):
        if (tensor_val[0][0].item() == -9999 or arg_name not in self.hf_model_fwd_args): return False
        return True

# Cell
class HF_BaseModelCallback(Callback):
    def after_pred(self):
        self.learn.pred = self.pred[0]

# Cell
@typedispatch
def show_results(x:HF_BaseInput, y, samples, outs, hf_tokenizer, skip_special_tokens=True,
                 ctxs=None, max_n=6, **kwargs):

    if ctxs is None: ctxs = get_empty_df(min(len(samples), max_n))

    res = L((TitledStr(hf_tokenizer.decode(inp, skip_special_tokens=skip_special_tokens).replace(hf_tokenizer.pad_token, '')), *s[1:])
            for inp, s in zip(x[0], samples))

    ctxs = show_batch[object](x, y, res, max_n=max_n, ctxs=ctxs, **kwargs)

    n_preds_per_input = len(outs[0])
    if (n_preds_per_input == 1):
        for i,ctx in enumerate(ctxs): ctx['target'] = outs[i][0]
    else:
        for pred_idx in range(n_preds_per_input):
            for i,ctx in enumerate(ctxs):  ctx[f'target{pred_idx+1}'] = outs[i][pred_idx]

    display_df(pd.DataFrame(ctxs))
    return ctxs