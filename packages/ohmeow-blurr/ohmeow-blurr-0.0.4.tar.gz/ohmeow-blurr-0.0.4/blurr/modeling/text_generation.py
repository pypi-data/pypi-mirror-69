# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02e_modeling-text-generation.ipynb (unless otherwise specified).

__all__ = ['HF_TextGenerationModelWrapper', 'text_gen_splitter']

# Cell
import ast, torch
from transformers import *
from fastai2.text.all import *

from ..data.all import *
from .core import *

# Cell
class HF_TextGenerationModelWrapper(HF_BaseModelWrapper):
    def forward(self, x):
        model_kwargs, n_inputs = {}, len(x)
        model_kwargs['input_ids'] = x[0]
        if (n_inputs > 1 and self._include_arg('attention_mask', x[1])): model_kwargs['attention_mask'] = x[1]
        if (n_inputs > 2 and self._include_arg('token_type_ids', x[2])): model_kwargs['token_type_ids'] = x[2]
        if (n_inputs > 3 and self._include_arg('decoder_input_ids', x[3])): model_kwargs['decoder_input_ids'] = x[3]

        return self.hf_model(**model_kwargs)

# Cell
def text_gen_splitter(m, arch):
    """Custom param splitter for text generation models"""
    model = m.hf_model if (hasattr(m, 'hf_model')) else m

    if (arch == 'bart'):
        groups = L(model.model.encoder,
                   model.model.decoder.embed_tokens,
                   model.model.decoder.embed_positions,
                   model.model.decoder.layers,
                   model.model.decoder.layernorm_embedding)

        return groups.map(params).filter(lambda el: len(el) > 0)

    raise ValueError('Invalid architecture')

# Cell
@typedispatch
def show_results(x:HF_TextGenerationInput, y, samples, outs, hf_tokenizer, skip_special_tokens=True,
                 generated_max_len=130, ctxs=None, max_n=6, **kwargs):
    res = L()
    for inp, trg, sample, pred in zip(x[0], y, samples, outs):
        txt_inp = hf_tokenizer.decode(inp, skip_special_tokens=skip_special_tokens).replace(hf_tokenizer.pad_token, '')
        txt_trg = hf_tokenizer.decode(trg, skip_special_tokens=skip_special_tokens).replace(hf_tokenizer.pad_token, '')
        txt_pred = pred[0].replace(hf_tokenizer.pad_token, '')[:generated_max_len]
        res.append((txt_inp, txt_trg, txt_pred))

    display_df(pd.DataFrame(res, columns=['text', 'target', 'prediction'])[:max_n])
    return ctxs

# Cell
@patch
def generate_text(self:Learner, inp, max_length=130, min_length=30, **kwargs):
    """Uses the built-in `generate` method to generate the text
    (see [here](https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.generate)
    for a list of arguments you can pass in)
    """
    # grab the huggingface tokenizer from the learner's dls.tfms
    hf_textblock_tfm = self.dls.tfms[0]
    hf_tokenizer = hf_textblock_tfm.hf_tokenizer
    add_prefix_space = hf_textblock_tfm.add_prefix_space

    input_ids = hf_tokenizer.encode(inp, add_prefix_space=add_prefix_space, return_tensors='pt')
    input_ids = input_ids.to(self.model.hf_model.device)

    gen_text = self.model.hf_model.generate(input_ids, max_length=max_length, min_length=min_length, **kwargs)

    outputs = [ hf_tokenizer.decode(txt, skip_special_tokens=True, clean_up_tokenization_spaces=False)
               for txt in gen_text ]

    return outputs