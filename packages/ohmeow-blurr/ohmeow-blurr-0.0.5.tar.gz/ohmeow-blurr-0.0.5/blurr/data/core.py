# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_data-core.ipynb (unless otherwise specified).

__all__ = ['HF_BaseInput', 'HF_TokenizerTransform', 'HF_BatchTransform', 'HF_TextBlock']

# Cell
from functools import reduce

import torch
from transformers import *
from fastai2.text.all import *

from ..utils import *

# Cell
class HF_BaseInput(list): pass

# Cell
class HF_TokenizerTransform(Transform):
    """huggingface friendly tokenization transfor."""
    def __init__(self, hf_arch, hf_tokenizer, title='text', **kwargs):

        store_attr(self, 'hf_arch, hf_tokenizer, title')
        self.add_prefix_space = hf_arch in ['gpt2', 'roberta', 'bart']

    def encodes(self, inp):
        """Supports both string and list[str] inputs (the later is common for token classification tasks).
        Returns the numericalized (token_ids) of the input so no need to run this through a Numericalization
        transform."""
        if (isinstance(inp, str)):
            toks = self.hf_tokenizer.tokenize(inp, add_prefix_space=self.add_prefix_space)
        else:
            toks = [sub_toks for entity in inp
                    for sub_toks in self.hf_tokenizer.tokenize(entity, add_prefix_space=self.add_prefix_space)]

        return tensor(self.hf_tokenizer.convert_tokens_to_ids(toks))

    def decodes(self, encoded_inp):
        """This will get called multiple times for a given encoded input because our batch transform will add
        other elements to it (e.g., attention_mask, token_type_ids, etc...) as required by the defined huggingface
        tokenizer and model.  If it can't decode it, return None."""
        try: return TitledStr(self.hf_tokenizer.decode(encoded_inp.cpu().numpy()))
        except: return None


# Cell
@typedispatch
def build_hf_input(task, tokenizer, a_tok_ids, b_tok_ids=None, targets=None,
                   max_length=512, pad_to_max_length=True, truncation_strategy='longest_first'):

    res = tokenizer.prepare_for_model(a_tok_ids, b_tok_ids,
                                       max_length=max_length, pad_to_max_length=pad_to_max_length,
                                       truncation_strategy=truncation_strategy, return_tensors='pt')

    input_ids = res['input_ids'][0]
    attention_mask = res['attention_mask'][0] if ('attention_mask' in res) else torch.tensor([-9999])
    token_type_ids = res['token_type_ids'][0] if ('token_type_ids' in res) else torch.tensor([-9999])

    return HF_BaseInput([input_ids, attention_mask, token_type_ids]), targets

# Cell
class HF_BatchTransform(Transform):
    """Handles everything you need to assemble a mini-batch of inputs and targets"""
    def __init__(self, hf_arch, hf_tokenizer, max_seq_len=512, truncation_strategy='longest_first', task=None,
                 **kwargs):

        self.hf_arch = hf_arch
        self.hf_tokenizer = hf_tokenizer
        store_attr(self, 'max_seq_len, truncation_strategy, task, kwargs')

    def encodes(self, samples):
        encoded_samples = []
        for idx, sample in enumerate(samples):
            if (isinstance(sample[0], tuple)):
                a_tok_ids, b_tok_ids = sample[0][0].tolist(), sample[0][1].tolist()
            else:
                a_tok_ids, b_tok_ids = sample[0].tolist(), None

            hf_base_input, targets = build_hf_input(self.task, self.hf_tokenizer,
                                                    a_tok_ids, b_tok_ids, sample[1:],
                                                    self.max_seq_len, True, self.truncation_strategy, **self.kwargs)

            encoded_samples.append((hf_base_input, *targets))

        return encoded_samples

# Cell
class HF_TextBlock(TransformBlock):
    def __init__(self, hf_arch, hf_tokenizer, hf_batch_tfm=None, max_seq_len=512, task=None, **kwargs):

        if (hf_batch_tfm is None):
            hf_batch_tfm = HF_BatchTransform(hf_arch, hf_tokenizer, max_seq_len, task=task, **kwargs)

        dl_type = SortedDL
        if (isinstance(task, ForConditionalGenerationTask)): dl_type=None

        return super().__init__(type_tfms=HF_TokenizerTransform(hf_arch, hf_tokenizer),
                                dl_type=dl_type, dls_kwargs={ 'before_batch': hf_batch_tfm })

# Cell
@typedispatch
def show_batch(x:HF_BaseInput, y, samples, hf_tokenizer, skip_special_tokens=True, ctxs=None, max_n=6, **kwargs):
    if ctxs is None: ctxs = get_empty_df(min(len(samples), max_n))

    samples = L((TitledStr(hf_tokenizer.decode(inp, skip_special_tokens=skip_special_tokens).replace(hf_tokenizer.pad_token, '')), *s[1:])
                for inp, s in zip(x[0], samples))

    ctxs = show_batch[object](x, y, samples, max_n=max_n, ctxs=ctxs, **kwargs)

    display_df(pd.DataFrame(ctxs))
    return ctxs