---
# ProtoAttend-specific params
candidate_batch_size: 1024
alpha: 0.5
sparsity_weight: 0.0001
sparsity_epsilon: 1.0e-06
confidence_weight: 1

# ProtoAttend-specific model parameters
d_intermediate: 256
d_val: 64
d_attention: 16


# Optimizer params
optimizer: Adam
optimizer_params:
  lr: 0.001
  betas:
  - 0.9
  - 0.999
  eps: 1.0e-08
  weight_decay: 0.01
  amsgrad: false

## Scheduler params
#scheduler: ReduceLROnPlateau
#scheduler_params:
#  monitor: val_loss
#  interval: epoch
#  frequency: 1
#  mode: min
#  factor: 0.7
#  patience: 3
#  verbose: true
#  threshold: 0.0001
#  threshold_mode: rel
#  cooldown: 0
#  min_lr: 0
#  eps: 1.0e-08

# PyTorch-lightning Trainer params
trainer_params:
  logger: true
  checkpoint_callback: true
  early_stop_callback: true
  default_root_dir: ./
  gradient_clip_val: 20.0
  gpus: 0
  accumulate_grad_batches: 4
  min_epochs: 5
  max_epochs: 100
  train_percent_check: 1.0
  val_percent_check: 1.0
  distributed_backend: null
  precision: 32
  auto_lr_find: false
  auto_scale_batch_size: false
  amp_level: 'O2'

# DataLoader params (only required if Datasets passed in instead of DataLoaders)
train_dataloader_params:
  batch_size: 128
  shuffle: true
  num_workers: 8
  pin_memory: true
  drop_last: false
val_dataloader_params:
  batch_size: 128
  shuffle: true
  num_workers: 8
  pin_memory: true
  drop_last: false
