{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nGroup Lasso regularization\n==========================\n\nThis example solves an inverse problem where the ground truth\ncoefficients (in orange) follow a group structure. In blue are\nthe recovered coefficients for group lasso with different values\nof the regularization parameter.\n\n\nThe group lasso regularization enters the optimization through\nits proximal operator, which is implemented in copt through the\nfunction prox of object :meth:`copt.utils.GroupL1`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import copt as cp\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import sparse\n\nnp.random.seed(0)\n\n# .. generate some data ..\nn_samples, n_features = 100, 100\ngroups = [np.arange(10 * i, 10 * i + 10) for i in range(10)]\n\n# .. construct a ground truth vector in which ..\n# .. group 4 and 5 are nonzero ..\nground_truth = np.zeros(n_features)\nground_truth[groups[4]] = 1\nground_truth[groups[5]] = 0.5\n\nmax_iter = 5000\nprint(\"#features\", n_features)\n\nA = sparse.rand(n_samples, n_features, density=0.2)\nsigma = 1.0\nb = A.dot(ground_truth) + sigma * np.random.randn(n_samples)\n\nnp.random.seed(0)\nn_samples = n_features\n\n# .. compute the step-size ..\nf = cp.utils.SquareLoss(A, b)\nstep_size = 1.0 / f.lipschitz\n\n# .. run the solver for different values ..\n# .. of the regularization parameter beta ..\nall_betas = [0, 1e-2, 1e-1, 0.2]\nall_trace_ls, all_trace_nols = [], []\nout_img = []\nfor i, beta in enumerate(all_betas):\n    print(\"beta = %s\" % beta)\n    G1 = cp.utils.GroupL1(beta, groups)\n\n    def loss(x):\n        return f(x) + G1(x)\n\n    x0 = np.zeros(n_features)\n    pgd = cp.minimize_proximal_gradient(\n        f.f_grad,\n        x0,\n        G1.prox,\n        jac=True,\n        max_iter=max_iter,\n        tol=1e-10,\n        trace_certificate=True,\n    )\n    out_img.append(pgd.x)\n\n\n# .. plot the results ..\nfig, ax = plt.subplots(2, 4, sharey=False)\nxlim = [0.02, 0.02, 0.1]\nmarkevery = [1000, 1000, 100, 100]\nfor i, beta in enumerate(all_betas):\n    ax[0, i].set_title(\"regularization=%s\" % beta)\n    ax[0, i].set_title(\"$regularization=%s\" % beta)\n    ax[0, i].plot(out_img[i])\n    ax[0, i].plot(ground_truth)\n    ax[0, i].set_ylim((-0.5, 1.5))\n    ax[0, i].set_xticks(())\n    ax[0, i].set_yticks(())\n\n    plot_tos, = ax[1, i].plot(\n        pgd.trace_certificate, lw=3, marker=\"o\", markevery=20, markersize=10\n    )\n\n    ax[1, i].set_xlabel(\"Iterations\")\n    ax[1, i].set_yscale(\"log\")\n    ax[1, i].set_ylim((1e-8, None))\n    ax[1, i].grid(True)\n\n\nax[1, 0].set_ylabel(\"certificate\")\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}