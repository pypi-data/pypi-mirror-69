{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nEstimating a sparse and low rank matrix\n=======================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(__doc__)\nimport numpy as np\nfrom scipy.sparse import linalg as splinalg\nimport matplotlib.pyplot as plt\nimport copt as cp\n\n# .. Generate synthetic data ..\nnp.random.seed(1)\n\nsigma_2 = 0.6\nN = 100\nd = 20\nblocks = np.array([2 * d / 10, 1 * d / 10, 1 * d / 10, 3 * d / 10, 3 * d / 10]).astype(\n    np.int\n)\nepsilon = 10 ** (-15)\n\nmu = np.zeros(d)\nSigma = np.zeros((d, d))\nblck = 0\nfor k in range(len(blocks)):\n    v = 2 * np.random.rand(int(blocks[k]), 1)\n    v = v * (abs(v) > 0.9)\n    Sigma[blck : blck + blocks[k], blck : blck + blocks[k]] = np.dot(v, v.T)\n    blck = blck + blocks[k]\nX = np.random.multivariate_normal(\n    mu, Sigma + epsilon * np.eye(d), N\n) + sigma_2 * np.random.randn(N, d)\nSigma_hat = np.cov(X.T)\n\nthreshold = 1e-5\nSigma[np.abs(Sigma) < threshold] = 0\nSigma[np.abs(Sigma) >= threshold] = 1\n\n# .. generate some data ..\n\nmax_iter = 5000\n\nn_features = np.multiply(*Sigma.shape)\nn_samples = n_features\nprint(\"#features\", n_features)\nA = np.random.randn(n_samples, n_features)\n\nsigma = 1.0\nb = A.dot(Sigma.ravel()) + sigma * np.random.randn(n_samples)\n\n# .. compute the step-size ..\ns = splinalg.svds(A, k=1, return_singular_vectors=False, tol=1e-3, maxiter=500)[0]\nf = cp.utils.HuberLoss(A, b)\nstep_size = 1.0 / f.lipschitz\n\n# .. run the solver for different values ..\n# .. of the regularization parameter beta ..\nall_betas = [0, 1e-3, 1e-2, 1e-1]\nall_trace_ls, all_trace_nols, all_trace_pdhg_nols, all_trace_pdhg = [], [], [], []\nall_trace_ls_time, all_trace_nols_time, all_trace_pdhg_nols_time, all_trace_pdhg_time = (\n    [],\n    [],\n    [],\n    [],\n)\nout_img = []\nfor i, beta in enumerate(all_betas):\n    print(\"beta = %s\" % beta)\n    G1 = cp.utils.TraceNorm(beta, Sigma.shape)\n    G2 = cp.utils.L1Norm(beta)\n\n    def loss(x):\n        return f(x) + G1(x) + G2(x)\n\n    cb_tosls = cp.utils.Trace()\n    x0 = np.zeros(n_features)\n    tos_ls = cp.minimize_three_split(\n        f.f_grad,\n        x0,\n        G2.prox,\n        G1.prox,\n        step_size=5 * step_size,\n        max_iter=max_iter,\n        tol=1e-14,\n        verbose=1,\n        callback=cb_tosls,\n        h_Lipschitz=beta,\n    )\n    trace_ls = np.array([loss(x) for x in cb_tosls.trace_x])\n    all_trace_ls.append(trace_ls)\n    all_trace_ls_time.append(cb_tosls.trace_time)\n\n    cb_tos = cp.utils.Trace()\n    x0 = np.zeros(n_features)\n    tos = cp.minimize_three_split(\n        f.f_grad,\n        x0,\n        G1.prox,\n        G2.prox,\n        step_size=step_size,\n        max_iter=max_iter,\n        tol=1e-14,\n        verbose=1,\n        line_search=False,\n        callback=cb_tos,\n    )\n    trace_nols = np.array([loss(x) for x in cb_tos.trace_x])\n    all_trace_nols.append(trace_nols)\n    all_trace_nols_time.append(cb_tos.trace_time)\n    out_img.append(tos.x)\n\n# .. plot the results ..\nf, ax = plt.subplots(2, 4, sharey=False)\nxlim = [0.02, 0.02, 0.1]\nfor i, beta in enumerate(all_betas):\n    ax[0, i].set_title(r\"$\\lambda=%s$\" % beta)\n    ax[0, i].set_title(r\"$\\lambda=%s$\" % beta)\n    ax[0, i].imshow(\n        out_img[i].reshape(Sigma.shape), interpolation=\"nearest\", cmap=plt.cm.gray_r\n    )\n    ax[0, i].set_xticks(())\n    ax[0, i].set_yticks(())\n\n    fmin = min(np.min(all_trace_ls[i]), np.min(all_trace_nols[i]))\n    plot_tos, = ax[1, i].plot(\n        all_trace_ls[i] - fmin, lw=4, marker=\"o\", markevery=100, markersize=10\n    )\n\n    plot_nols, = ax[1, i].plot(\n        all_trace_nols[i] - fmin, lw=4, marker=\"h\", markevery=100, markersize=10\n    )\n\n    ax[1, i].set_xlabel(\"Iterations\")\n    ax[1, i].set_yscale(\"log\")\n    ax[1, i].set_ylim((1e-15, None))\n    ax[1, i].set_xlim((0, 2000))\n    ax[1, i].grid(True)\n\n\nplt.gcf().subplots_adjust(bottom=0.15)\nplt.figlegend(\n    (plot_tos, plot_nols),\n    (\"TOS with line search\", \"TOS without line search\"),\n    ncol=5,\n    scatterpoints=1,\n    loc=(-0.00, -0.0),\n    frameon=False,\n    bbox_to_anchor=[0.05, 0.01],\n)\n\nax[1, 0].set_ylabel(\"Objective minus optimum\")\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}