{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nTotal variation regularization\n==============================\n\nComparison of solvers with total variation regularization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import copt as cp\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom scipy import misc\nfrom scipy import sparse\nfrom scipy.sparse import linalg as splinalg\n\nnp.random.seed(0)\n\nimg = misc.face(gray=True).astype(np.float)\n# resize\nimg = np.array(Image.fromarray(img).resize((153, 115)))\nimg = img.astype(np.float) / img.max()\n\nn_rows, n_cols = img.shape\nn_features = n_rows * n_cols\nn_samples = n_features\nmax_iter = 2000\n\n# .. compute blurred and noisy image ..\nA = sparse.load_npz(\"data/blur_matrix.npz\")\nb = A.dot(img.ravel()) + 0 * np.random.randn(n_samples)\n\nnp.random.seed(0)\nn_samples = n_features\n\n# .. compute the step-size ..\ns = splinalg.svds(A, k=1, return_singular_vectors=False, tol=1e-3, maxiter=500)[0]\nf = cp.utils.SquareLoss(A, b)\nstep_size = 1.0 / f.lipschitz\n\n\ndef loss(x, pen):\n    x_mat = x.reshape((n_rows, n_cols))\n    tmp1 = np.abs(np.diff(x_mat, axis=0))\n    tmp2 = np.abs(np.diff(x_mat, axis=1))\n    return f(x) + pen * (tmp1.sum() + tmp2.sum())\n\n\n# .. run the solver for different values ..\n# .. of the regularization parameter beta ..\nall_betas = [0, 1e-6, 5e-6]\nall_trace_ls, all_trace_nols, all_trace_pdhg, out_img = [], [], [], []\nall_trace_ls_time, all_trace_nols_time, all_trace_pdhg_time = [], [], []\nfor i, beta in enumerate(all_betas):\n    print(\"Iteration %s, beta %s\" % (i, beta))\n\n    def g_prox(x, gamma, pen=beta):\n        return cp.tv_prox.prox_tv1d_cols(gamma * pen, x, n_rows, n_cols)\n\n    def h_prox(x, gamma, pen=beta):\n        return cp.tv_prox.prox_tv1d_rows(gamma * pen, x, n_rows, n_cols)\n\n    cb_adatos = cp.utils.Trace()\n    x0 = np.zeros(n_features)\n    adatos = cp.minimize_three_split(\n        f.f_grad,\n        x0,\n        g_prox,\n        h_prox,\n        step_size=10 * step_size,\n        max_iter=max_iter,\n        tol=1e-14,\n        verbose=1,\n        callback=cb_adatos,\n        h_Lipschitz=beta,\n    )\n    trace_ls = [loss(x, beta) for x in cb_adatos.trace_x]\n    all_trace_ls.append(trace_ls)\n    all_trace_ls_time.append(cb_adatos.trace_time)\n    out_img.append(adatos.x.reshape(img.shape))\n\n    cb_tos = cp.utils.Trace()\n    x0 = np.zeros(n_features)\n    cp.minimize_three_split(\n        f.f_grad,\n        x0,\n        g_prox,\n        h_prox,\n        step_size=step_size,\n        max_iter=max_iter,\n        tol=1e-14,\n        verbose=1,\n        callback=cb_tos,\n        line_search=False,\n    )\n    trace_nols = [loss(x, beta) for x in cb_tos.trace_x]\n    all_trace_nols.append(trace_nols)\n    all_trace_nols_time.append(cb_tos.trace_time)\n\n    cb_pdhg = cp.utils.Trace()\n    x0 = np.zeros(n_features)\n    cp.minimize_primal_dual(\n        f.f_grad,\n        x0,\n        g_prox,\n        h_prox,\n        callback=cb_pdhg,\n        max_iter=max_iter,\n        step_size=step_size,\n        step_size2=(1.0 / step_size) / 2,\n        tol=0,\n    )\n    trace_pdhg = np.array([loss(x, beta) for x in cb_pdhg.trace_x])\n    all_trace_pdhg.append(trace_pdhg)\n    all_trace_pdhg_time.append(cb_pdhg.trace_time)\n\n# .. plot the results ..\nf, ax = plt.subplots(2, 3, sharey=False)\nxlim = [0.02, 0.02, 0.1]\nfor i, beta in enumerate(all_betas):\n    ax[0, i].set_title(r\"$\\lambda=%s$\" % beta)\n    ax[0, i].imshow(out_img[i], interpolation=\"nearest\", cmap=plt.cm.gray)\n    ax[0, i].set_xticks(())\n    ax[0, i].set_yticks(())\n\n    fmin = min(np.min(all_trace_ls[i]), np.min(all_trace_pdhg[i]))\n    scale = all_trace_ls[i][0] - fmin\n    plot_tos, = ax[1, i].plot(\n        (all_trace_ls[i] - fmin) / scale,\n        \"--\",\n        lw=2,\n        marker=\"o\",\n        markevery=400,\n        markersize=7,\n    )\n\n    plot_tos_nols, = ax[1, i].plot(\n        (all_trace_nols[i] - fmin) / scale,\n        lw=2,\n        marker=\"<\",\n        markevery=400,\n        markersize=7,\n    )\n\n    plot_pdhg, = ax[1, i].plot(\n        (all_trace_pdhg[i] - fmin) / scale,\n        \"--\",\n        lw=2,\n        marker=\"^\",\n        markevery=400,\n        markersize=7,\n    )\n\n    ax[1, i].set_xlabel(\"Iterations\")\n    ax[1, i].set_yscale(\"log\")\n    ax[1, i].set_ylim((1e-14, None))\n    ax[1, i].set_xlim((0, 1500))\n    ax[1, i].grid(True)\n\n\nplt.gcf().subplots_adjust(bottom=0.25)\nplt.figlegend(\n    (plot_tos, plot_tos_nols, plot_pdhg),\n    (\n        \"Adaptive three operator splitting\",\n        \"three operator splitting\",\n        \"primal-dual hybrid gradient\",\n    ),\n    \"lower center\",\n    ncol=2,\n    scatterpoints=1,\n    frameon=False,\n)\n\nax[1, 0].set_ylabel(\"Objective minus optimum\")\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}