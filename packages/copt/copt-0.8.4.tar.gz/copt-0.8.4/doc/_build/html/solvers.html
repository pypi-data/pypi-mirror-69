
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Solvers &#8212; copt  documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/gallery.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/custom.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <script type="text/javascript" src="_static/copybutton.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"argmin": "\\DeclareMathOperator*{\\argmin}{\\mathbf{arg\\,min}}", "argmax": "\\DeclareMathOperator*{\\argmin}{\\mathbf{arg\\,max}}", "bs": "\\newcommand{\\bs}[1]{\\boldsymbol{#1}}"}}, "tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]]}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="copt.minimize_proximal_gradient" href="generated/copt.minimize_proximal_gradient.html" />
    <link rel="prev" title="COPT: a Python library for Constrained OPTimization" href="index.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="solvers">
<span id="proximal-gradient"></span><h1>Solvers<a class="headerlink" href="#solvers" title="Permalink to this headline">¶</a></h1>
<div class="section" id="id1">
<h2>Proximal-Gradient<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/copt.minimize_proximal_gradient.html#copt.minimize_proximal_gradient" title="copt.minimize_proximal_gradient"><code class="xref py py-obj docutils literal notranslate"><span class="pre">copt.minimize_proximal_gradient</span></code></a>(fun, x0[, …])</p></td>
<td><p>Proximal gradient descent.</p></td>
</tr>
</tbody>
</table>
<p>The proximal-gradient method <a class="reference internal" href="#bt2009" id="id2"><span>[BT2009]</span></a>, <a class="reference internal" href="#n2013" id="id3"><span>[N2013]</span></a> is a method to solve problems of the form</p>
<div class="math notranslate nohighlight">
\[\argmin_{\bs{x} \in \mathbb{R}^d} f(\bs{x}) + g(\bs{x})\]</div>
<p>where $f$ is a differentiable function for which we have access to its gradient and $g$ is a potentially non-smooth function for which we have access to its proximal operator.</p>
<div class="admonition-examples admonition">
<p class="admonition-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="auto_examples/plot_group_lasso.html#sphx-glr-auto-examples-plot-group-lasso-py"><span class="std std-ref">Group Lasso regularization</span></a></p></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">References</p>
<dl class="citation">
<dt class="label" id="bt2009"><span class="brackets"><a class="fn-backref" href="#id2">BT2009</a></span></dt>
<dd><p>Beck, Amir, and Marc Teboulle. <a class="reference external" href="https://pdfs.semanticscholar.org/e7a7/5a379a515197e058102d985cd80f4f047c04.pdf">“Gradient-based algorithms with applications to signal recovery.”</a> Convex optimization in signal processing and communications (2009)</p>
</dd>
<dt class="label" id="n2013"><span class="brackets"><a class="fn-backref" href="#id3">N2013</a></span></dt>
<dd><p>Nesterov, Yu. <a class="reference external" href="https://doi.org/10.1007/s10107-012-0629-5">“Gradient methods for minimizing composite functions.”</a> Mathematical Programming 140.1 (2013): 125-161.</p>
</dd>
</dl>
</div>
</div>
<div class="section" id="primal-dual-hybrid-gradient">
<h2>Primal-dual hybrid gradient<a class="headerlink" href="#primal-dual-hybrid-gradient" title="Permalink to this headline">¶</a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/copt.minimize_primal_dual.html#copt.minimize_primal_dual" title="copt.minimize_primal_dual"><code class="xref py py-obj docutils literal notranslate"><span class="pre">copt.minimize_primal_dual</span></code></a>(f_grad, x0[, …])</p></td>
<td><p>Primal-dual hybrid gradient splitting method.</p></td>
</tr>
</tbody>
</table>
<p>The primal-dual hybrid gradient method <a class="reference internal" href="#c2013" id="id4"><span>[C2013]</span></a> <a class="reference internal" href="#v2013" id="id5"><span>[V2013]</span></a> <a class="reference internal" href="#cp2016" id="id6"><span>[CP2016]</span></a> is a method to solve problems of the form</p>
<div class="math notranslate nohighlight">
\[\argmin_{\bs{x} \in \mathbb{R}^d} f(\bs{x}) + g(\bs{x}) + h(\bs{A}\bs{x})\]</div>
<p>where $f$ is a differentiable function for which we have access to its gradient and $g$ and $h$ are potentially non-smooth functions for which we have access to their proximal operator.</p>
<div class="admonition-examples admonition">
<p class="admonition-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="auto_examples/proximal_splitting/plot_tv_deblurring.html#sphx-glr-auto-examples-proximal-splitting-plot-tv-deblurring-py"><span class="std std-ref">Total variation regularization</span></a></p></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">References</p>
<dl class="citation">
<dt class="label" id="c2013"><span class="brackets"><a class="fn-backref" href="#id4">C2013</a></span></dt>
<dd><p>Condat, Laurent. “A primal–dual splitting method for convex optimization involving Lipschitzian, proximable and linear composite terms.” Journal of Optimization Theory and Applications 158.2 (2013): 460-479.</p>
</dd>
<dt class="label" id="v2013"><span class="brackets"><a class="fn-backref" href="#id5">V2013</a></span></dt>
<dd><p>Vũ, Bằng Công. “A splitting algorithm for dual monotone inclusions involving cocoercive operators.” Advances in Computational Mathematics 38.3 (2013)</p>
</dd>
<dt class="label" id="cp2016"><span class="brackets"><a class="fn-backref" href="#id6">CP2016</a></span></dt>
<dd><p>Chambolle, Antonin, and Thomas Pock. “An introduction to continuous optimization for imaging.” Acta Numerica 25 (2016)</p>
</dd>
</dl>
</div>
</div>
<div class="section" id="three-operator-splitting">
<h2>Three-operator splitting<a class="headerlink" href="#three-operator-splitting" title="Permalink to this headline">¶</a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/copt.minimize_three_split.html#copt.minimize_three_split" title="copt.minimize_three_split"><code class="xref py py-obj docutils literal notranslate"><span class="pre">copt.minimize_three_split</span></code></a>(f_grad, x0[, …])</p></td>
<td><p>Davis-Yin three operator splitting method.</p></td>
</tr>
</tbody>
</table>
<p>The three operator splitting <a class="reference internal" href="#dy2017" id="id7"><span>[DY2017]</span></a> <a class="reference internal" href="#pg2018" id="id8"><span>[PG2018]</span></a> is a method to solve problems of the form</p>
<div class="math notranslate nohighlight">
\[\argmin_{\bs{x} \in \mathbb{R}^d} f(\bs{x}) + g(\bs{x}) + h(\bs{x})\]</div>
<p>where $f$ is a differentiable function for which we have access to its gradient and $g$ and $h$ are potentially non-smooth functions for which we have access to their proximal operator.</p>
<div class="admonition-examples admonition">
<p class="admonition-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="auto_examples/proximal_splitting/plot_sparse_nuclear_norm.html#sphx-glr-auto-examples-proximal-splitting-plot-sparse-nuclear-norm-py"><span class="std std-ref">Estimating a sparse and low rank matrix</span></a></p></li>
<li><p><a class="reference internal" href="auto_examples/proximal_splitting/plot_tv_deblurring.html#sphx-glr-auto-examples-proximal-splitting-plot-tv-deblurring-py"><span class="std std-ref">Total variation regularization</span></a></p></li>
<li><p><a class="reference internal" href="auto_examples/proximal_splitting/plot_overlapping_group_lasso.html#sphx-glr-auto-examples-proximal-splitting-plot-overlapping-group-lasso-py"><span class="std std-ref">Group lasso with overlap</span></a></p></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">References</p>
<dl class="citation">
<dt class="label" id="dy2017"><span class="brackets"><a class="fn-backref" href="#id7">DY2017</a></span></dt>
<dd><p>Davis, Damek, and Wotao Yin. <a class="reference external" href="https://doi.org/10.1007/s11228-017-0421-z">“A three-operator splitting scheme and
its optimization applications.”</a> Set-Valued and Variational
Analysis, 2017.</p>
</dd>
<dt class="label" id="pg2018"><span class="brackets"><a class="fn-backref" href="#id8">PG2018</a></span></dt>
<dd><p>Pedregosa, Fabian, and Gauthier Gidel. <a class="reference external" href="https://arxiv.org/abs/1804.02339">“Adaptive Three Operator
Splitting.”</a> Proceedings of the 35th
International Conference on Machine Learning, 2018.</p>
</dd>
</dl>
</div>
</div>
<div class="section" id="frank-wolfe">
<span id="id9"></span><h2>Frank-Wolfe<a class="headerlink" href="#frank-wolfe" title="Permalink to this headline">¶</a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/copt.minimize_frank_wolfe.html#copt.minimize_frank_wolfe" title="copt.minimize_frank_wolfe"><code class="xref py py-obj docutils literal notranslate"><span class="pre">copt.minimize_frank_wolfe</span></code></a>(fun, x0, lmo[, …])</p></td>
<td><p>Frank-Wolfe algorithm.</p></td>
</tr>
</tbody>
</table>
<p>The Frank-Wolfe (FW) or conditional gradient algorithm <a class="reference internal" href="#j2003" id="id10"><span>[J2003]</span></a>, <a class="reference internal" href="#p2018" id="id11"><span>[P2018]</span></a>, <a class="reference internal" href="#panj2018" id="id12"><span>[PANJ2018]</span></a> is a method for constrained optimization. It can solve problems of the form</p>
<div class="math notranslate nohighlight">
\[\argmin_{\bs{x} \in \mathcal{D}} f(\bs{x})\]</div>
<p>where <span class="math notranslate nohighlight">\(f\)</span> is a differentiable function for which we have access to its gradient and <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is a compact set for which we have access to its linear minimization oracle (lmo). This is a routine that given a vector <span class="math notranslate nohighlight">\(\bs{u}\)</span> returns a solution to</p>
<div class="math notranslate nohighlight">
\[\argmin_{\bs{x} \in D}\, \langle\bs{u}, \bs{x}\rangle~.\]</div>
<p>Contrary to other constrained optimization algorithms like projected gradient descent, the Frank-Wolfe algorithm does not require access to a projection, hence why it is sometimes referred to as a projection-free algorithm. It instead relies exclusively on the linear minimization oracle described above.</p>
<p>The Frank-Wolfe algorithm is implemented in this library in the method <a class="reference internal" href="generated/copt.minimize_frank_wolfe.html#copt.minimize_frank_wolfe" title="copt.minimize_frank_wolfe"><code class="xref py py-meth docutils literal notranslate"><span class="pre">copt.minimize_frank_wolfe()</span></code></a>. As most other methods it takes as argument an objective function to minimize, but unlike most other methods, it requires access to a <em>linear minimization oracle</em>, which is a routine that for a given $d$-dimensional vector <span class="math notranslate nohighlight">\(\bs{u}\)</span> solves the linear problems  <span class="math notranslate nohighlight">\(\argmin_{\bs{z} \in D}\, \langle \bs{u}, \bs{z}\rangle\)</span>.</p>
<p>At each iteration, the Frank-Wolfe algorithm uses the linear minimization oracle to identify the vertex <span class="math notranslate nohighlight">\(\bs{s}_t\)</span> that correlates most with the negative gradient. Then next iterate <span class="math notranslate nohighlight">\(\bs{x}^+\)</span> is constructed as a convex combination of the current iterate <span class="math notranslate nohighlight">\(\bs{x}\)</span> and the newly acquired vertex <span class="math notranslate nohighlight">\(\bs{s}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{x}^+ = (1 - \gamma)\boldsymbol{x} + \gamma \boldsymbol{s}\]</div>
<p>The step-size <span class="math notranslate nohighlight">\(\gamma\)</span> can be chosen by different strategies:</p>
<blockquote>
<div><ul>
<li><p><strong>Backtracking line-search</strong>. This is the default option and corresponds to the keyword argument <code class="code docutils literal notranslate"><span class="pre">step_size=&quot;backtracking&quot;</span></code> This is typically the fastest and simplest method, if unsure, use this option.</p></li>
<li><p><strong>Demyanov-Rubinov step-size</strong>. This is a step-size of the form</p>
<div class="math notranslate nohighlight">
\[\gamma = \langle \nabla f(\bs{x}), \bs{s} - \bs{x}\rangle / (L \|\bs{s} - \bs{x}\|^2)~.\]</div>
<p>This step-size typically performs well but has the drawback that it requires knowledge of the Lipschitz constant of <span class="math notranslate nohighlight">\(\nabla f\)</span>. This step-size can be used with the keyword argument <code class="code docutils literal notranslate"><span class="pre">step_size=&quot;DR&quot;</span></code>. In this case the Lipschitz
constant <span class="math notranslate nohighlight">\(L\)</span> needs to be specified through the keyword argument <code class="code docutils literal notranslate"><span class="pre">lipschitz</span></code>. For example, if the lipschitz constant is 0.1, then the signature should include <code class="code docutils literal notranslate"><span class="pre">step_size=&quot;DR&quot;,</span> <span class="pre">lipschitz=0.1</span></code>.</p>
</li>
<li><p><strong>Oblivious step-size</strong>. This is the very simple step-size of the form</p>
<div class="math notranslate nohighlight">
\[\gamma = \frac{2}{t+2}~,\]</div>
<p>where <span class="math notranslate nohighlight">\(t\)</span> is the number of iterations. This step-size is oblivious since it doesn’t use any previous information of the objective. It typically performs worst than the alternatives, but is simple to implement and can be competitive in the case in the case of noisy objectives.</p>
</li>
</ul>
</div></blockquote>
<div class="admonition-examples admonition">
<p class="admonition-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="auto_examples/frank_wolfe/plot_sparse_benchmark.html#sphx-glr-auto-examples-frank-wolfe-plot-sparse-benchmark-py"><span class="std std-ref">Benchmark of Frank-Wolfe variants for sparse logistic regression</span></a></p></li>
<li><p><a class="reference internal" href="auto_examples/frank_wolfe/plot_vertex_overlap.html#sphx-glr-auto-examples-frank-wolfe-plot-vertex-overlap-py"><span class="std std-ref">Update Direction Overlap in Frank-Wolfe</span></a></p></li>
<li><p><a class="reference internal" href="auto_examples/frank_wolfe/plot_sparse_benchmark_pairwise.html#sphx-glr-auto-examples-frank-wolfe-plot-sparse-benchmark-pairwise-py"><span class="std std-ref">Benchmark of Pairwise Frank-Wolfe variants for sparse logistic regression</span></a></p></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">References:</p>
<dl class="citation">
<dt class="label" id="j2003"><span class="brackets"><a class="fn-backref" href="#id10">J2003</a></span></dt>
<dd><p>Jaggi, Martin. <a class="reference external" href="http://proceedings.mlr.press/v28/jaggi13-supp.pdf">“Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization.”</a> ICML 2013.</p>
</dd>
<dt class="label" id="p2018"><span class="brackets"><a class="fn-backref" href="#id11">P2018</a></span></dt>
<dd><p>Pedregosa, Fabian <a class="reference external" href="http://fa.bianp.net/blog/2018/notes-on-the-frank-wolfe-algorithm-part-i/">“Notes on the Frank-Wolfe Algorithm”</a>, 2018</p>
</dd>
<dt class="label" id="panj2018"><span class="brackets"><a class="fn-backref" href="#id12">PANJ2018</a></span></dt>
<dd><p>Pedregosa, Fabian, Armin Askari, Geoffrey Negiar, and Martin Jaggi. <a class="reference external" href="https://arxiv.org/pdf/1806.05123.pdf">“Step-Size Adaptivity in Projection-Free Optimization.”</a> arXiv:1806.05123 (2018).</p>
</dd>
<dt class="label" id="lj2015"><span class="brackets">LJ2015</span></dt>
<dd><p>Lacoste-Julien, Simon, and Martin Jaggi. <a class="reference external" href="https://arxiv.org/pdf/1511.05932.pdf">“On the global linear convergence of Frank-Wolfe optimization variants.”</a> Advances in Neural Information Processing Systems. 2015.</p>
</dd>
</dl>
</div>
</div>
<div class="section" id="stochastic-methods">
<span id="id13"></span><h2>Stochastic methods<a class="headerlink" href="#stochastic-methods" title="Permalink to this headline">¶</a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/copt.minimize_saga.html#copt.minimize_saga" title="copt.minimize_saga"><code class="xref py py-obj docutils literal notranslate"><span class="pre">copt.minimize_saga</span></code></a>(f_deriv, A, b, x0, step_size)</p></td>
<td><p>Stochastic average gradient augmented (SAGA) algorithm.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/copt.minimize_svrg.html#copt.minimize_svrg" title="copt.minimize_svrg"><code class="xref py py-obj docutils literal notranslate"><span class="pre">copt.minimize_svrg</span></code></a>(f_deriv, A, b, x0, step_size)</p></td>
<td><p>Stochastic average gradient augmented (SAGA) algorithm.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/copt.minimize_vrtos.html#copt.minimize_vrtos" title="copt.minimize_vrtos"><code class="xref py py-obj docutils literal notranslate"><span class="pre">copt.minimize_vrtos</span></code></a>(f_deriv, A, b, x0, step_size)</p></td>
<td><p>Variance-reduced three operator splitting (VRTOS) algorithm.</p></td>
</tr>
</tbody>
</table>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><p><a class="reference internal" href="auto_examples/plot_saga_vs_svrg.html#sphx-glr-auto-examples-plot-saga-vs-svrg-py"><span class="std std-ref">SAGA vs SVRG</span></a></p></li>
</ul>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="_static/logo.png" alt="Logo"/>
            </a></p>
<h1 class="logo"><a href="index.html">copt</a></h1>



<p class="blurb">Optimization in Python</p>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Solvers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id1">Proximal-Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="#primal-dual-hybrid-gradient">Primal-dual hybrid gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="#three-operator-splitting">Three-operator splitting</a></li>
<li class="toctree-l2"><a class="reference internal" href="#frank-wolfe">Frank-Wolfe</a></li>
<li class="toctree-l2"><a class="reference internal" href="#stochastic-methods">Stochastic methods</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="loss_functions.html">Loss and regularization functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_examples/index.html">Example Gallery</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">Utility functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="citing.html">Citing</a></li>
</ul>


<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      
      
      
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.2.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.11</a>
      
      |
      <a href="_sources/solvers.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    
    <a href="https://github.com/openopt/copt" class="github">
        <img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub"  class="github"/>
    </a>
    

    
  </body>
</html>